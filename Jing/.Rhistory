beta0 <- 50
beta1 <- 10
sig2 <- 16
# By repeatedly simulating regression data this way, we  |
# realize a process of *hypthetical repeated sampling*,  |
# which allows us to make sense of the theoretical       |
# properties of statistical procedures.                  |
n.rep = 500
alpha = 0.05
# Empty vectors for storing values
intercepts = numeric(length=n.rep)
slopes = numeric(length=n.rep)
b1.ci.sample = numeric(length=n.rep)
sig2.ci.sample = numeric(length=n.rep)
b1.null.reject.sample.test1 = numeric(length=n.rep)
b1.null.reject.sample.test2 = numeric(length=n.rep)
# Critical values
chisq.crit.lo <- qchisq(1-alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.hi <- qchisq(alpha/2, df=n-2, lower.tail=FALSE)
t.crit = qt(alpha/2, df=n-2, lower.tail=FALSE)
S.xx = sum((x-mean(x))^2)
for (rep in 1:n.rep){
y = beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
y.bar = mean(y)
x.bar = mean(x)
S.xy = sum((y-mean(y))*(x-mean(x)))
# Estimates of beta0 and beta1
b1.hat = S.xy/S.xx
b0.hat = y.bar - b1.hat*x.bar
intercepts[rep] = b0.hat
slopes[rep] = b1.hat
# Corrected Sum of squares
SS.T = sum((y-mean(y))^2)
# Residual Sum of squares
SS.Res = SS.T - b1.hat*S.xy
# Unbiased estimator of error variance
sig2.hat = SS.Res / (n-2)
MS.Res = sig2.hat
# Standard error of b1.hat
se.b1.hat = sqrt(MS.Res / S.xx)
# Margin of error for testing
m.err = t.crit*se.b1.hat
# CI for b1 estimates
b1.ci.sample[rep] = (abs(b1.hat - beta1) <= m.err)
# CI for sig2.hat (MS.Res) estimates
sig2.ci.sample[rep] = ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res / chisq.crit.lo)
# test for the hypotheses H0: beta1 = 10 vs H0: beta1 ≠ 10
t0 = (b1.hat - 10) / se.b1.hat
b1.null.reject.sample.test1[rep] = (abs(t0) > t.crit)
# test for the hypotheses H0: beta1 = 9 vs H0: beta1 ≠ 9 given beta1 = 10
t0 = (b1.hat - 9) / se.b1.hat
b1.null.reject.sample.test2[rep] = (abs(t0) > t.crit)
}
# Part A: At each repetition of the simulation,          |
# calculate and record the values of the fitted          |
# regression coefficients for intercept and slope. Once  |
# you have completed 500 simulations, calculate the      |
# sample mean and sample variance of each set of values  |
# that you collected. Are the numerical values of those  |
# sample means and sample variances consistent with the  |
# numerical values that you would obtain from the        |
# expected values and variance formulas of the least-    |
# squares estimates for intercept and slope?             |
sam.mean.intercepts = mean(intercepts)
sam.mean.slopes = mean(slopes)
sam.var.intercepts = var(intercepts)
sam.var.slopes = var(slopes)
c(sam.mean.slopes, beta1)
# [1]  9.540233 10.000000
c(sam.mean.intercepts, beta0)
# [1] 50.75446 50.00000
c(sam.var.slopes, sig2/S.xx)
# [1] 0.1217734 0.1122807
c(sam.var.intercepts, sig2*(1/n + x.bar**2/S.xx))
for (rep in 1:100000){
y = beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
y.bar = mean(y)
x.bar = mean(x)
S.xy = sum((y-mean(y))*(x-mean(x)))
# Estimates of beta0 and beta1
b1.hat = S.xy/S.xx
b0.hat = y.bar - b1.hat*x.bar
intercepts[rep] = b0.hat
slopes[rep] = b1.hat
# Corrected Sum of squares
SS.T = sum((y-mean(y))^2)
# Residual Sum of squares
SS.Res = SS.T - b1.hat*S.xy
# Unbiased estimator of error variance
sig2.hat = SS.Res / (n-2)
MS.Res = sig2.hat
# Standard error of b1.hat
se.b1.hat = sqrt(MS.Res / S.xx)
# Margin of error for testing
m.err = t.crit*se.b1.hat
# CI for b1 estimates
b1.ci.sample[rep] = (abs(b1.hat - beta1) <= m.err)
# CI for sig2.hat (MS.Res) estimates
sig2.ci.sample[rep] = ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res / chisq.crit.lo)
# test for the hypotheses H0: beta1 = 10 vs H0: beta1 ≠ 10
t0 = (b1.hat - 10) / se.b1.hat
b1.null.reject.sample.test1[rep] = (abs(t0) > t.crit)
# test for the hypotheses H0: beta1 = 9 vs H0: beta1 ≠ 9 given beta1 = 10
t0 = (b1.hat - 9) / se.b1.hat
b1.null.reject.sample.test2[rep] = (abs(t0) > t.crit)
}
# Part A: At each repetition of the simulation,          |
# calculate and record the values of the fitted          |
# regression coefficients for intercept and slope. Once  |
# you have completed 500 simulations, calculate the      |
# sample mean and sample variance of each set of values  |
# that you collected. Are the numerical values of those  |
# sample means and sample variances consistent with the  |
# numerical values that you would obtain from the        |
# expected values and variance formulas of the least-    |
# squares estimates for intercept and slope?             |
sam.mean.intercepts = mean(intercepts)
sam.mean.slopes = mean(slopes)
sam.var.intercepts = var(intercepts)
sam.var.slopes = var(slopes)
c(sam.mean.slopes, beta1)
# [1]  9.540233 10.000000
c(sam.mean.intercepts, beta0)
# [1] 50.75446 50.00000
c(sam.var.slopes, sig2/S.xx)
# [1] 0.1217734 0.1122807
c(sam.var.intercepts, sig2*(1/n + x.bar**2/S.xx))
for (rep in 1:n.rep){
y = beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
y.bar = mean(y)
x.bar = mean(x)
S.xy = sum((y-mean(y))*(x-mean(x)))
# Estimates of beta0 and beta1
b1.hat = S.xy/S.xx
b0.hat = y.bar - b1.hat*x.bar
intercepts[rep] = b0.hat
slopes[rep] = b1.hat
# Corrected Sum of squares
SS.T = sum((y-mean(y))^2)
# Residual Sum of squares
SS.Res = SS.T - b1.hat*S.xy
# Unbiased estimator of error variance
sig2.hat = SS.Res / (n-2)
MS.Res = sig2.hat
# Standard error of b1.hat
se.b1.hat = sqrt(MS.Res / S.xx)
# Margin of error for testing
m.err = t.crit*se.b1.hat
# CI for b1 estimates
b1.ci.sample[rep] = (abs(b1.hat - beta1) <= m.err)
# CI for sig2.hat (MS.Res) estimates
sig2.ci.sample[rep] = ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res / chisq.crit.lo)
# test for the hypotheses H0: beta1 = 10 vs H0: beta1 ≠ 10
t0 = (b1.hat - 10) / se.b1.hat
b1.null.reject.sample.test1[rep] = (abs(t0) > t.crit)
# test for the hypotheses H0: beta1 = 9 vs H0: beta1 ≠ 9 given beta1 = 10
t0 = (b1.hat - 9) / se.b1.hat
b1.null.reject.sample.test2[rep] = (abs(t0) > t.crit)
}
# Part A: At each repetition of the simulation,          |
# calculate and record the values of the fitted          |
# regression coefficients for intercept and slope. Once  |
# you have completed 500 simulations, calculate the      |
# sample mean and sample variance of each set of values  |
# that you collected. Are the numerical values of those  |
# sample means and sample variances consistent with the  |
# numerical values that you would obtain from the        |
# expected values and variance formulas of the least-    |
# squares estimates for intercept and slope?             |
sam.mean.intercepts = mean(intercepts)
sam.mean.slopes = mean(slopes)
sam.var.intercepts = var(intercepts)
n <- length(x)
beta0 <- 50
beta1 <- 10
sig2 <- 16
# By repeatedly simulating regression data this way, we  |
# realize a process of *hypthetical repeated sampling*,  |
# which allows us to make sense of the theoretical       |
# properties of statistical procedures.                  |
n.rep = 500
alpha = 0.05
# Empty vectors for storing values
intercepts = numeric(length=n.rep)
slopes = numeric(length=n.rep)
b1.ci.sample = numeric(length=n.rep)
sig2.ci.sample = numeric(length=n.rep)
b1.null.reject.sample.test1 = numeric(length=n.rep)
b1.null.reject.sample.test2 = numeric(length=n.rep)
# Critical values
chisq.crit.lo <- qchisq(1-alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.hi <- qchisq(alpha/2, df=n-2, lower.tail=FALSE)
t.crit = qt(alpha/2, df=n-2, lower.tail=FALSE)
S.xx = sum((x-mean(x))^2)
for (rep in 1:n.rep){
y = beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
y.bar = mean(y)
x.bar = mean(x)
S.xy = sum((y-mean(y))*(x-mean(x)))
# Estimates of beta0 and beta1
b1.hat = S.xy/S.xx
b0.hat = y.bar - b1.hat*x.bar
intercepts[rep] = b0.hat
slopes[rep] = b1.hat
# Corrected Sum of squares
SS.T = sum((y-mean(y))^2)
# Residual Sum of squares
SS.Res = SS.T - b1.hat*S.xy
# Unbiased estimator of error variance
sig2.hat = SS.Res / (n-2)
MS.Res = sig2.hat
# Standard error of b1.hat
se.b1.hat = sqrt(MS.Res / S.xx)
# Margin of error for testing
m.err = t.crit*se.b1.hat
# CI for b1 estimates
b1.ci.sample[rep] = (abs(b1.hat - beta1) <= m.err)
# CI for sig2.hat (MS.Res) estimates
sig2.ci.sample[rep] = ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res / chisq.crit.lo)
# test for the hypotheses H0: beta1 = 10 vs H0: beta1 ≠ 10
t0 = (b1.hat - 10) / se.b1.hat
b1.null.reject.sample.test1[rep] = (abs(t0) > t.crit)
# test for the hypotheses H0: beta1 = 9 vs H0: beta1 ≠ 9 given beta1 = 10
t0 = (b1.hat - 9) / se.b1.hat
b1.null.reject.sample.test2[rep] = (abs(t0) > t.crit)
}
# Part A: At each repetition of the simulation,          |
# calculate and record the values of the fitted          |
# regression coefficients for intercept and slope. Once  |
# you have completed 500 simulations, calculate the      |
# sample mean and sample variance of each set of values  |
# that you collected. Are the numerical values of those  |
# sample means and sample variances consistent with the  |
# numerical values that you would obtain from the        |
# expected values and variance formulas of the least-    |
# squares estimates for intercept and slope?             |
sam.mean.intercepts = mean(intercepts)
sam.mean.slopes = mean(slopes)
sam.var.intercepts = var(intercepts)
sam.var.slopes = var(slopes)
c(sam.mean.slopes, beta1)
# [1]  9.540233 10.000000
c(sam.mean.intercepts, beta0)
# [1] 50.75446 50.00000
c(sam.var.slopes, sig2/S.xx)
# [1] 0.1217734 0.1122807
c(sam.var.intercepts, sig2*(1/n + x.bar**2/S.xx))
freq.b1 = sum(b1.ci.sample)/n.rep
freq.b1
# Expected Value = 0.05
#                                                        |
# Part E: At each repetition of the simulation, carry    |
# out a t test for the hypotheses                        |
#             H0: beta1 = 9 vs H0: beta1 ≠ 9             |
# controlling for type I error at level 0.05; record     |
# whether the test rejects or fails to reject H0. Once   |
# you have completed 500 simulations, propose an         |
# approximate value of the power of the test when the    |
# true value of the slope is beta1=10.                   |
power.test = sum(b1.null.reject.sample.test2)/n.rep
power.test
x <- seq(from=1, to=10, by=1.0)
x <- seq(from=1, to=10, by=1.0)
n <- length(x)
beta0 <- 50
beta1 <- 10
sig2 <- 16
# By repeatedly simulating regression data this way, we  |
# realize a process of *hypthetical repeated sampling*,  |
# which allows us to make sense of the theoretical       |
# properties of statistical procedures.                  |
n.rep = 500
alpha = 0.05
# Empty vectors for storing values
intercepts = numeric(length=n.rep)
slopes = numeric(length=n.rep)
b1.ci.sample = numeric(length=n.rep)
sig2.ci.sample = numeric(length=n.rep)
b1.null.reject.sample.test1 = numeric(length=n.rep)
b1.null.reject.sample.test2 = numeric(length=n.rep)
# Critical values
chisq.crit.lo <- qchisq(1-alpha/2, df=n-2, lower.tail=FALSE)
chisq.crit.hi <- qchisq(alpha/2, df=n-2, lower.tail=FALSE)
t.crit = qt(alpha/2, df=n-2, lower.tail=FALSE)
S.xx = sum((x-mean(x))^2)
for (rep in 1:n.rep){
y = beta0 + beta1*x + rnorm(n, mean=0, sd = sqrt(sig2))
y.bar = mean(y)
x.bar = mean(x)
S.xy = sum((y-mean(y))*(x-mean(x)))
# Estimates of beta0 and beta1
b1.hat = S.xy/S.xx
b0.hat = y.bar - b1.hat*x.bar
intercepts[rep] = b0.hat
slopes[rep] = b1.hat
# Corrected Sum of squares
SS.T = sum((y-mean(y))^2)
# Residual Sum of squares
SS.Res = SS.T - b1.hat*S.xy
# Unbiased estimator of error variance
sig2.hat = SS.Res / (n-2)
MS.Res = sig2.hat
# Standard error of b1.hat
se.b1.hat = sqrt(MS.Res / S.xx)
# Margin of error for testing
m.err = t.crit*se.b1.hat
# CI for b1 estimates
b1.ci.sample[rep] = (abs(b1.hat - beta1) <= m.err)
# CI for sig2.hat (MS.Res) estimates
sig2.ci.sample[rep] = ((n-2)*MS.Res / chisq.crit.hi <= sig2) & (sig2 <= (n-2)*MS.Res / chisq.crit.lo)
# test for the hypotheses H0: beta1 = 10 vs H0: beta1 ≠ 10
t0 = (b1.hat - 10) / se.b1.hat
b1.null.reject.sample.test1[rep] = (abs(t0) > t.crit)
# test for the hypotheses H0: beta1 = 9 vs H0: beta1 ≠ 9 given beta1 = 10
t0 = (b1.hat - 9) / se.b1.hat
b1.null.reject.sample.test2[rep] = (abs(t0) > t.crit)
}
# Part A: At each repetition of the simulation,          |
# calculate and record the values of the fitted          |
# regression coefficients for intercept and slope. Once  |
# you have completed 500 simulations, calculate the      |
# sample mean and sample variance of each set of values  |
# that you collected. Are the numerical values of those  |
# sample means and sample variances consistent with the  |
# numerical values that you would obtain from the        |
# expected values and variance formulas of the least-    |
# squares estimates for intercept and slope?             |
sam.mean.intercepts = mean(intercepts)
sam.mean.slopes = mean(slopes)
sam.var.intercepts = var(intercepts)
sam.var.slopes = var(slopes)
c(sam.mean.slopes, beta1)
# [1]  9.540233 10.000000
c(sam.mean.intercepts, beta0)
# [1] 50.75446 50.00000
c(sam.var.slopes, sig2/S.xx)
# [1] 0.1217734 0.1122807
c(sam.var.intercepts, sig2*(1/n + x.bar**2/S.xx))
freq.b1 = sum(b1.ci.sample)/n.rep
freq.b1
# calculate and record the values of the upper and lower |
# bounds of a 95% confidence interval for error          |
# variance; record whether the confidence interval       |
# surrounds the true value of the error variance,        |
# sig2 = 16. Once you have completed 500 simulations,    |
# calculate the relative frequency that the confidence   |
# interval surrounds the true value of the error         |
# variance. Is the numerical value of that relative      |
# frequency consistent with what you would expect?       |
# Explain.                                               |
freq.sig2 = sum(sig2.ci.sample)/n.rep
freq.sig2
# [1] 0.954
# Expected Value = 0.95
#                                                        |
# Part D: At each repetition of the simulation, carry    |
# out a t test for the hypotheses                        |
#             H0: beta1 = 10 vs H0: beta1 ≠ 10           |
# controlling for type I error at level 0.05; record     |
# whether the test rejects or fails to reject H0. Once   |
# you have completed 500 simulations, calculate the      |
# relative frequency that the test rejects H0.           |
freq.reject.h0 = sum(b1.null.reject.sample.test1)/n.rep
freq.reject.h0
# Expected Value = 0.05
#                                                        |
# Part E: At each repetition of the simulation, carry    |
# out a t test for the hypotheses                        |
#             H0: beta1 = 9 vs H0: beta1 ≠ 9             |
# controlling for type I error at level 0.05; record     |
# whether the test rejects or fails to reject H0. Once   |
# you have completed 500 simulations, propose an         |
# approximate value of the power of the test when the    |
# true value of the slope is beta1=10.                   |
power.test = sum(b1.null.reject.sample.test2)/n.rep
power.test
library(caret)
library(dplyr)
library(ISLR)
setwd("~/Documents/github/sys6018-competition-house-prices/Jing")
library(caret)
library(dplyr)
library(ISLR)
library(purrr)
library(ggplot2)
library(randomForest)
library(gridExtra)
library(class)
train <- read.csv('train.csv')
test <- read.csv('test.csv')
test$SalePrice <- NA
train$isTrain <- 1
test$isTrain <- 0
house <- rbind(train, test)
# fill in missing values for house (train+test) ---------------------------
house_missing <- data.frame(index = names(house), missing_count = colSums(sapply(house, is.na)))
house_missing$index[which(house_missing$missing_count>0)]
# fill in missing LotFrontage and GarageArea
house$LotFrontage[which(is.na(house$LotFrontage))] <- mean(house$LotFrontage, na.rm=TRUE)
house$GarageArea[which(is.na(house$GarageArea))] <- mean(house$GarageArea, na.rm=TRUE)
house[,c(7,26,31,32,33,34,36,43,58,59,61,64,65,73:75)] <-
lapply(house[,c(7,26,31,32,33,34,36,43,58,59,61,64,65,73:75)], as.character)
house[,c(7,26,31,32,33,34,36,43,58,59,61,64,65,73:75)][is.na(house[,c(7,26,31,32,33,34,36,43,58,59,61,64,65,73:75)])] <- 'None'
house[,c(7,26,31,32,33,34,36,43,58,59,61,64,65,73:75)] <-
lapply(house[,c(7,26,31,32,33,34,36,43,58,59,61,64,65,73:75)], as.factor)
# fill in missing MasVnrArea,GarageYrBlt,BsmtFinSF1,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,BsmtFullBath,BsmtHalfBath
house[,c(27,35,37:39,48,49,60)][is.na(house[,c(27,35,37:39,48,49,60)])] <- 0
# function for get mode https://www.tutorialspoint.com/r/r_mean_median_mode.htm
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
# missing values for MSZoning,Utilities,Exterior1st,Exterior2nd,KitchenQual,Functional,GarageCars,SaleType
modeList <- c(3,10,24,25,54,56,62,79)
for (i in 1:length(modeList)) {
house[is.na(house[,modeList[i]]),modeList[i]] <- getmode(house[,modeList[i]])
}
# separate back to train and test
train <- house[house$isTrain==1,]
train <- subset(train,select=-isTrain)
test <- house[house$isTrain==0,]
test <- subset(test,select=-c(isTrain,SalePrice))
cor_data <- train %>% select_if(negate(is.factor))
corr <- as.data.frame(round(cor(cor_data),2))
corr_df <- data.frame(var = colnames(cor_data), cor = corr$SalePrice)
corr_df[order(-corr_df$cor),][2:6,]
p1 <- ggplot(train,aes(OverallQual,SalePrice))+geom_point()
p2 <- ggplot(train,aes(GrLivArea,SalePrice))+geom_point()
p3 <- ggplot(train,aes(GarageCars,SalePrice))+geom_point()
p4 <- ggplot(train,aes(GarageArea,SalePrice))+geom_point()
p5 <- ggplot(train,aes(TotalBsmtSF,SalePrice))+geom_point()
grid.arrange(p1,p2,p3,p4,p5)
train <- train[train$GrLivArea<=4500,]
train <- train[train$TotalBsmtSF<4000,]
# Sampling for cross validation -------------------------------------------
set.seed(222)
ind <- sample(1:nrow(train), size=0.8*nrow(train))
training_cv <- subset(train[ind,],select=-Id)
validation <- subset(train[-ind,],select=-Id)
View(train)
ncols(train)
ncol(train)
View(house)
knn_house <- house
knn_house <- select(knn_house, c(Id,OverallQual,Neighborhood,GrLivArea,GarageCars,ExterQual,TotalBsmtSF,
X1stFlrSF,GarageArea,BsmtFinSF1,X2ndFlrSF,BsmtQual,KitchenQual,
YearBuilt,FullBath,LotArea,isTrain,SalePrice))
knn_house <- select(knn_house, c(Id,OverallQual,GrLivArea,GarageCars,TotalBsmtSF,
X1stFlrSF,GarageArea,BsmtFinSF1,X2ndFlrSF,isTrain,SalePrice))
View(knn_house)
knn_house_nonfactor <- knn_house %>%  select_if(negate(is.factor))
knn_house_factor <- data.frame(Id=knn_house$Id,knn_house %>% select_if(is.factor))
View(knn_house_nonfactor)
knn_house_nonfactor[,2:9] <- as.data.frame(lapply(knn_house_nonfactor[,2:9], normalize))
normalize <- function(x) {
norm <- ((x - min(x))/(max(x) - min(x)))
return (norm)
}
knn_house_nonfactor[,2:9] <- as.data.frame(lapply(knn_house_nonfactor[,2:9], normalize))
View(knn_house_factor)
View(knn_house_nonfactor)
for (i in 1:length(colnames(knn_house_factor))) {
if (is.factor(knn_house_factor[,i])) {
levels <- unique(knn_house_factor[,i])
for (j in 1:length(levels)) {
knn_house_factor[paste(colnames(knn_house_factor)[i],levels[j],sep='')] <-
ifelse(knn_house_factor[,i]==levels[j],1,0)
}
}
}
knn_house_factor <- knn_house_factor[,-c(2:3)]
knn_final <- merge(knn_house_factor,knn_house_nonfactor,by='Id')
knn_final <- merge(knn_house_factor,knn_house_nonfactor,by='Id')
normalize <- function(x) {
norm <- ((x - min(x))/(max(x) - min(x)))
return (norm)
}
knn_house <- house
knn_house <- select(knn_house, c(Id,OverallQual,Neighborhood,GrLivArea,GarageCars,ExterQual,TotalBsmtSF,
X1stFlrSF,GarageArea,BsmtFinSF1,X2ndFlrSF,BsmtQual,KitchenQual,
YearBuilt,FullBath,LotArea,isTrain,SalePrice))
knn_house <- select(knn_house, c(Id,OverallQual,GrLivArea,GarageCars,TotalBsmtSF,
X1stFlrSF,GarageArea,BsmtFinSF1,X2ndFlrSF,isTrain,SalePrice))
knn_house_nonfactor <- knn_house %>%  select_if(negate(is.factor))
knn_house_factor <- data.frame(Id=knn_house$Id,knn_house %>% select_if(is.factor))
knn_house_nonfactor[,2:9] <- as.data.frame(lapply(knn_house_nonfactor[,2:9], normalize))
for (i in 1:length(colnames(knn_house_factor))) {
if (is.factor(knn_house_factor[,i])) {
levels <- unique(knn_house_factor[,i])
for (j in 1:length(levels)) {
knn_house_factor[paste(colnames(knn_house_factor)[i],levels[j],sep='')] <-
ifelse(knn_house_factor[,i]==levels[j],1,0)
}
}
}
knn_house_factor <- knn_house_factor[,-c(2:3)]
knn_final <- merge(knn_house_factor,knn_house_nonfactor,by='Id')
knn_train <- knn_final[knn_final$isTrain==1,]
knn_train <- subset(knn_train,select=-c(Id,isTrain))
View(knn_house_nonfactor)
knn_house_factor <- data.frame(Id=knn_house$Id,knn_house %>% select_if(is.factor))
View(knn_house_factor)
knn_house_nonfactor[,2:9] <- as.data.frame(lapply(knn_house_nonfactor[,2:9], normalize))
knn_final <- merge(knn_house_factor,knn_house_nonfactor,by='Id')
View(knn_final)
knn_train <- knn_final[knn_final$isTrain==1,]
knn_train <- subset(knn_train,select=-c(Id,isTrain))
knn_test <- knn_final[knn_final$isTrain==0,]
knn_test <- subset(knn_test,select=-c(Id,isTrain,SalePrice))
View(knn_test)
View(knn_train)
View(knn_test)
